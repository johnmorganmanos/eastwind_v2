{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab07fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pint.util:Redefining 'strain' (<class 'pint.delegates.txt_defparser.plain.UnitDefinition'>)\n",
      "WARNING:pint.util:Redefining 'Ïµ' (<class 'pint.delegates.txt_defparser.plain.UnitDefinition'>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from glob import glob\n",
    "from scipy import signal\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.signal import detrend\n",
    "from tqdm import tqdm\n",
    "import obspy\n",
    "from obspy.signal.trigger import recursive_sta_lta, plot_trigger, trigger_onset\n",
    "import pickle as pkl\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import pyTMD\n",
    "import numpy as np\n",
    "        \n",
    "import pyTMD.io\n",
    "import pyTMD.predict\n",
    "import pyTMD.tools\n",
    "import pyTMD.utilities\n",
    "import timescale.time\n",
    "\n",
    "np.float_ = np.float64\n",
    "import dascore as dc\n",
    "\n",
    "def sintela_to_datetime(sintela_times):\n",
    "    '''\n",
    "    returns an array of datetime.datetime \n",
    "    ''' \n",
    "    \n",
    "    days1970 = datetime.datetime.date(datetime.datetime(1970, 1, 1)).toordinal()\n",
    "\n",
    "    # Vectorize everything\n",
    "    converttime = np.vectorize(datetime.datetime.fromordinal)\n",
    "    addday_lambda = lambda x : datetime.timedelta(days=x)\n",
    "    adddays = np.vectorize(addday_lambda )\n",
    "    \n",
    "    day = days1970 + sintela_times/1e6/60/60/24\n",
    "    thisDateTime = converttime(np.floor(day).astype(int))\n",
    "    dayFraction = day-np.floor(day)\n",
    "    thisDateTime = thisDateTime + adddays(dayFraction)\n",
    "\n",
    "    return thisDateTime\n",
    "\n",
    "def preprocessing_step(file):\n",
    "\n",
    "    # Load data #\n",
    "    f = h5py.File(file)\n",
    "    attrs = f['Acquisition'].attrs\n",
    "    data = f['Acquisition']['Raw[0]']['RawData'][:]\n",
    "    this_time = f['Acquisition']['Raw[0]']['RawDataTime'][:]\n",
    "    times = sintela_to_datetime(this_time)\n",
    "    x = np.linspace(0,data.shape[1],data.shape[1]) * attrs['SpatialSamplingInterval']\n",
    "\n",
    "\n",
    "    fs = attrs['PulseRate'] #sample rate\n",
    "    bp_top = 200\n",
    "    bp_bottom = 1\n",
    "    downsample_rate = int(fs/bp_top)\n",
    "\n",
    "    #filter by freq\n",
    "    sos = signal.butter(10, [bp_bottom,bp_top], 'bp', fs=fs, output='sos')\n",
    "    filtered = signal.sosfiltfilt(sos, data, axis=0)\n",
    "\n",
    "    #FK filter\n",
    "    new_format_times = time_fixer_4_fk(times)\n",
    "    dims = ('time', 'distance')\n",
    "    patch = dc.Patch(data=filtered, coords=dict(time=[np.datetime64(i) for i in new_format_times], distance=x), dims=dims)\n",
    "    filt_cutoffs = np.array([0, 400, 8000, np.inf])\n",
    "\n",
    "    fk_filtered = patch.slope_filter(filt=filt_cutoffs)\n",
    "    fk_filtered_data = np.array(fk_filtered.data)\n",
    "\n",
    "\n",
    "    #normalize\n",
    "    data_normed_filtered = chan_norm(fk_filtered_data)\n",
    "\n",
    "    if file[-9:-7] != '00':\n",
    "        time_start = times[0] - datetime.timedelta(seconds=times[0].second, microseconds=times[0].microsecond)\n",
    "        forward_step = np.arange(time_start, times[0], 500).shape[0]\n",
    "        data_locator = np.array([int(i) for i in (this_time-this_time[0])/500]) + forward_step -1\n",
    "    else:\n",
    "        data_locator = np.array([int(i) for i in (this_time-this_time[0])/500])\n",
    "\n",
    "\n",
    "\n",
    "    # this_time = np.arange(0,int(fs*60))* 500 + this_time[0]\n",
    "    # print(this_time)\n",
    "    # print(filled_data.shape)\n",
    "    # print(first_filler.shape)\n",
    "    filled_data = np.zeros((int(fs*60),data_normed_filtered.shape[1]))\n",
    "    filled_times = np.zeros((int(fs*60)), dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if data_locator[0]==0:\n",
    "        filled_data[data_locator] = data_normed_filtered\n",
    "\n",
    "    else:\n",
    "        first_filler = np.array([data_normed_filtered[:,0]]*data_locator[0])\n",
    "        filled_data[:first_filler.shape[0],:] = first_filler\n",
    "    \n",
    "    filt_filled_data = filled_data\n",
    "\n",
    "    filled_times[data_locator] = times\n",
    "    # filled_data[data_locator] = data_normed_filtered.T\n",
    "\n",
    "    # filt_filled_data = filled_data\n",
    "\n",
    "    ## Downsample \n",
    "    filled_times = filled_times[::downsample_rate]\n",
    "    filt_filled_data = filt_filled_data[::downsample_rate,::5] #Skip every 5th channel\n",
    "    new_dict = dict(attrs)\n",
    "    new_dict['PulseRate'] = new_dict['PulseRate']/downsample_rate\n",
    "\n",
    "\n",
    "    return filt_filled_data, filled_times, new_dict\n",
    "\n",
    "def foo(a):\n",
    "    t = mdates.num2date(a)\n",
    "    ms = str(t.microsecond)[:1]\n",
    "    res = f\"{t.hour:02}:{t.minute:02}:{t.second:02}.{ms}\"\n",
    "    return res\n",
    "\n",
    "def chan_norm(das_data):\n",
    "    data_normed = (das_data - np.mean(das_data, axis=0))/np.std(das_data, axis=0).T\n",
    "    # data_normed_all_axis = (data_normed.T - np.mean(data_normed, axis=1))/np.std(data_normed, axis=1)   \n",
    "\n",
    "    return data_normed\n",
    "\n",
    "class DataStats:\n",
    "    def __init__(self, data, attrs, times):\n",
    "        self.sampling_rate = attrs[\"PulseRate\"]\n",
    "        self.npts = data.shape[0]\n",
    "        self.starttime = times[0]\n",
    "        # self.starttime.isoformat\n",
    "\n",
    "class DAS:\n",
    "    def __init__(self, id, data, attrs, times):\n",
    "        self.id = id\n",
    "        self.data = data\n",
    "        self.stats = DataStats(data, attrs, times)\n",
    "\n",
    "\n",
    "def obspy_stream_from_das(data, attrs,times):\n",
    "    stats_default = {\n",
    "        'network':'eastwind',\n",
    "        'station':'',\n",
    "        'location':'',\n",
    "        'channel':'DAS',\n",
    "        'starttime':times[0].strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "        'endtime':times[-1].strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "        'sampling_rate':attrs['PulseRate'],\n",
    "        'delta':1/attrs['PulseRate'],\n",
    "        'npts':0,\n",
    "        'calib':1.0\n",
    "    }\n",
    "\n",
    "    streams = []\n",
    "    for n,i in enumerate(data.T):\n",
    "        tr = obspy.Trace(data=i,header=stats_default)\n",
    "        # tr.stats.station = f'Channel {n}'\n",
    "        tr.stats.npts = len(i)\n",
    "\n",
    "        st = obspy.Stream(tr)\n",
    "        streams.append(st)\n",
    "    return streams\n",
    "\n",
    "def parallel_event_finding(dummy):\n",
    "    times_all,attrs,channel,all_data = dummy\n",
    "    trigger_on=3.5, \n",
    "    trigger_off=1.2\n",
    "    \n",
    "    DAS_channel = DAS(channel, all_data, attrs, times_all)\n",
    "\n",
    "\n",
    "    cft = recursive_sta_lta(DAS_channel.data, int(1 * attrs['PulseRate']), int(10 * attrs['PulseRate']))\n",
    "    trigger_times = trigger_onset(cft, trigger_on, trigger_off)\n",
    "    trigger_times_list = [trigger_times]     \n",
    "\n",
    "    trigger_times_list_datetime = []\n",
    "\n",
    "    for trigs_samp_time in trigger_times_list:\n",
    "        trigger_times_list_datetime.append(times_all[trigs_samp_time])\n",
    "    return {str(channel): trigger_times_list_datetime}\n",
    "\n",
    "\n",
    "def time_fixer_4_fk(times):\n",
    "    times_copy = times.copy()\n",
    "    new_format_times = []\n",
    "    for i in times_copy:\n",
    "        if round(i.microsecond/500)*500 == 1000000:\n",
    "            microsecond = 0\n",
    "            second = i.second + 1\n",
    "            minute = i.minute\n",
    "            hour = i.hour\n",
    "            day = i.day\n",
    "            if second == 60:\n",
    "                second = 0\n",
    "                minute = minute+1\n",
    "\n",
    "            if minute == 60:\n",
    "                minute = 0\n",
    "                hour=hour+1\n",
    "            \n",
    "            if hour == 24:\n",
    "                hour = 0\n",
    "                day = day+1\n",
    "            \n",
    "        else:\n",
    "            microsecond = int(round(i.microsecond/500)*500)\n",
    "            second = i.second\n",
    "            minute = i.minute\n",
    "            hour = i.hour\n",
    "            day = i.day\n",
    "\n",
    "        new_format_times.append(datetime.datetime(year=i.year,\n",
    "                                                month=i.month,\n",
    "                                                day=day,\n",
    "                                                hour=hour,\n",
    "                                                minute=minute,\n",
    "                                                second=second,\n",
    "                                                microsecond=microsecond))\n",
    "    return new_format_times\n",
    "\n",
    "\n",
    "import pathlib\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn.mass_downloader import (\n",
    "    GlobalDomain,\n",
    "    Restrictions,\n",
    "    MassDownloader,\n",
    ")\n",
    "from obspy.core import AttribDict\n",
    "from pyproj import Proj\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from quakemigrate import QuakeScan, Trigger\n",
    "from quakemigrate.io import Archive, read_stations\n",
    "from quakemigrate.lut import compute_traveltimes\n",
    "from quakemigrate.signal.onsets import STALTAOnset\n",
    "from quakemigrate.signal.pickers import GaussianPicker\n",
    "\n",
    "import emcee\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8651314",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f703d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc32df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Priors ###\n",
    "top_lim = 1400000\n",
    "bottom_lim = 1360000\n",
    "\n",
    "right_lim = 440000\n",
    "left_lim = 420000\n",
    "\n",
    "depth_lim = -1000\n",
    "height_lim = 100\n",
    "\n",
    "wavespeed_min = 100\n",
    "wavespeed_max = 5000\n",
    "\n",
    "### Functions ###\n",
    "def log_prior(theta, picks):\n",
    "    s_x,s_y,s_z,t_offset,c = theta\n",
    "\n",
    "    min_offset = np.array(picks[1]).min() - 5\n",
    "    max_offset = np.array(picks[1]).min() + 0\n",
    "    if left_lim < s_x < right_lim and bottom_lim < s_y < top_lim and depth_lim < s_z < height_lim and min_offset < t_offset < max_offset and wavespeed_min < c < wavespeed_max:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def log_likelihood(theta, picks):\n",
    "    s_x,s_y,s_z,t_offset,c = theta\n",
    "    chans_of_picks = np.array(picks[0], dtype=int)\n",
    "\n",
    "    distance = np.sqrt((channel_locations[chans_of_picks,0]-s_x)**2 + (channel_locations[chans_of_picks,1]-s_y)**2 + (channel_locations[chans_of_picks,2]-s_z)**2)   \n",
    "    \n",
    "\n",
    "    model = distance/c + t_offset\n",
    "\n",
    "    best_line_coeffs = np.poly1d(np.polyfit(chans_of_picks, picks[1], 5))\n",
    "    fit_line = np.polyval(best_line_coeffs, chans_of_picks)\n",
    "    pick_residual = picks[1] - fit_line\n",
    "    sigma = np.std(pick_residual)\n",
    "\n",
    "    sigma2 = sigma**2\n",
    "    #model =  t_offset + np.linalg.norm(channel_locations[chans_of_picks,:] - np.array([s_x, s_y]), axis=0) / c\n",
    "    ll = - 0.5 * np.sum((model - picks[1])**2 / sigma2 + np.log(sigma2))\n",
    "\n",
    "    return ll\n",
    "\n",
    "def log_probability(theta, picks):\n",
    "    lp = log_prior(theta, picks)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fcb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eastwind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
